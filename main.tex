\documentclass[11pt]{article}

% ---------- Page and typography ----------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{microtype}

% ---------- Math ----------
\usepackage{amsmath,amssymb,amsthm,mathtools,bm}
\usepackage{bbm} % for \mathbbm{1} indicator

% ---------- Lists, figures, links ----------
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
% Packages (only if not already loaded)
\usepackage{graphicx}
\usepackage{float} % for [H] placement if you want it

% ---------- Short commands ----------
% Sets and common objects
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% Probability
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Law}{\mathcal{L}}
\newcommand{\1}{\mathbbm{1}}

% Calligraphic letters (use as needed)
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cB}{\mathcal{B}}

% Notation helpers
\newcommand{\eps}{\varepsilon}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\given}{\,\middle|\,}
\newcommand{\st}{\,:\,}

% Norms and absolute values
\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\ip}[2]{\langle #1,#2\rangle}

% SDE / stochastic calculus
\newcommand{\dW}{\dd W_t}
\newcommand{\dB}{\dd B_t}
\newcommand{\dt}{\dd t}
\newcommand{\Ito}{It\^o}

% ---------- Exercise numbering as x.y (chapter.exercise) ----------
\newcounter{chap}
\newcounter{ex}[chap]
\renewcommand{\theex}{\arabic{chap}.\arabic{ex}}

% Set current chapter number (resets exercise counter)
\newcommand{\setchapter}[1]{%
  \setcounter{chap}{#1}%
  \setcounter{ex}{0}%
}

% Set the next exercise number within the current chapter
% Example: after \setchapter{2}, \setexercise{3} makes the next exercise be 2.3
\newcommand{\setexercise}[1]{%
  \setcounter{ex}{\numexpr#1-1\relax}%
}

% Exercise environment: optional title
\newenvironment{exercise}[1][]%
{%
  \refstepcounter{ex}%
  \section*{Exercise \theex\if\relax\detokenize{#1}\relax\else\ (#1)\fi}%
}%
{}


% Solution environment
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% ---------- Title info ----------
\title{Independent study - Stochastic Differential Equations}
\author{Johan J\"onsson}
\date{January 2026}

\begin{document}

\maketitle
\section*{Introduction}
This document collects a selection of exercises I worked through from Bernt \O ksendal, \emph{Stochastic Differential Equations: An Introduction with Applications}. These solutions were completed as part of my MATH 199 independent study with Professor Ang at UC San Diego. The document serves as a brief record of the topics and problem-solving work I carried out during the course.



% =======================
% Example (Chapter 2, Exercise 9)
% =======================
\setchapter{2}

% Force next one to be 2.9
\setexercise{9}
\begin{exercise}
To illustrate that the (finite-dimensional) distributions alone do not give all the information regarding the continuity properties of a process, consider the following example:

Let $(\Omega,\mathcal{F},P)=([0,\infty),\mathcal{B},\mu)$ where $\mathcal{B}$ denotes the Borel $\sigma$-algebra on $[0,\infty)$ and $\mu$ is a probability measure on $[0,\infty)$ with no mass on single points. Define
\[
X_t(\omega)=
\begin{cases}
1 & \text{if } t=\omega,\\
0 & \text{otherwise}
\end{cases}
\]
and
\[
Y_t(\omega)=0 \qquad \text{for all } (t,\omega)\in[0,\infty)\times[0,\infty).
\]
Prove that $\{X_t\}$ and $\{Y_t\}$ have the same distributions and that $X_t$ is a version of $Y_t$. And yet we have that $t\mapsto Y_t(\omega)$ is continuous for all $\omega$, while $t\mapsto X_t(\omega)$ is discontinuous for all $\omega$.

\end{exercise}

\begin{solution}
By definition 2.2.2 we have that $\{X_t\}$ and $\{Y_t\}$ the same finite-dimensional distribution if
\[
\bP(\{ \omega : X_t(\omega)=Y_t(\omega)\})=1, \, \forall t
\]
we then can in fact show this is true for our case so that will be how we show that they equal in distribution.
\[
\bP(\{ \omega : X_t(\omega)=Y_t(\omega)\})=1-\bP(\{ \omega : X_t(\omega)\neq Y_t(\omega)\})
\]
Then we use that
\[
\bP(\{ \omega : X_t(\omega)\neq Y_t(\omega)\})=\bP(\{ \omega : X_t(\omega)\neq 0\})=\bP(\{ \omega : X_t(\omega)= 1\})=\bP(\{ t\})=0
\]
This holds since it had no mass in a single point. This means that for any $t$ we have that $\bP(\{ \omega : X_t(\omega)\neq Y_t(\omega)\})=0$. This gives that
\[
\bP(\{ \omega : X_t(\omega)=Y_t(\omega)\})=1-\bP(\{ \omega : X_t(\omega)\neq Y_t(\omega)\})=1-0=1, \, \forall t
\]
Specifically we proved something stronger which is that $\{X_t\}$ is a version of $\{Y_t\}$. 
\end{solution}

% =======================
% Chapter 5, Exercise 1
% =======================
\setchapter{5}
\setexercise{1}
\begin{exercise}
Verify that the given processes solve the given corresponding stochastic differential equations ($B_t$ denotes $1$-dimensional Brownian motion).

\begin{enumerate}[label=(\roman*)]
\item $X_t = e^{B_t}$ solves
\[
dX_t = \frac{1}{2}X_t\,dt + X_t\,dB_t.
\]

\item $X_t = \dfrac{B_t}{1+t}$, $B_0=0$ solves
\[
dX_t = -\frac{1}{1+t}X_t\,dt + \frac{1}{1+t}\,dB_t; \qquad X_0=0.
\]

\item $X_t = \sin(B_t)$ with $B_0=a \in \bigl(-\frac{\pi}{2},\frac{\pi}{2}\bigr)$ solves
\[
dX_t = -\frac{1}{2}X_t\,dt + \sqrt{1-X_t^2}\,dB_t
\qquad \text{for } t < \inf\{s>0: B_s \notin \bigl[-\tfrac{\pi}{2},\tfrac{\pi}{2}\bigr]\}.
\]

\item $(X_1(t),X_2(t)) = (t, e^{t}B_t)$ solves
\[
\begin{bmatrix}
dX_1\\
dX_2
\end{bmatrix}
=
\begin{bmatrix}
1\\
X_2
\end{bmatrix}dt
+
\begin{bmatrix}
0\\
e^{X_1}
\end{bmatrix}dB_t.
\]

\item $(X_1(t),X_2(t)) = (\cosh(B_t),\sinh(B_t))$ solves
\[
\begin{bmatrix}
dX_1\\
dX_2
\end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix}
X_1\\
X_2
\end{bmatrix}dt
+
\begin{bmatrix}
X_2\\
X_1
\end{bmatrix}dB_t.
\]
\end{enumerate}
\end{exercise}

\begin{solution}

\begin{enumerate}[label=(\roman*)]
\item 

We begin to observe that 
\[
X_t=g(t,B_t)=e^{B_t} \implies \frac{\partial g}{\partial t}(t,x)=0, \, \frac{\partial g}{\partial x}(t,B_t)=e^{B_t}, \, \frac{\partial^2 g}{\partial x^2}(t,B_t)= e^{B_t} 
\]

\[
dX_t=\frac{\partial g}{\partial t}(t,B_t)\,dt+\frac{\partial g}{\partial x}(t,B_t)\,dB_t+\frac{1}{2}\frac{\partial^2 g}{\partial x^2}(t,B_t)\cdot(dB_t)^2=
\]
\[
e^{B_t}dB_t+ \frac{1}{2} e^{B_t}  \cdot(dB_t)^2 = e^{B_t}dB_t+\frac{1}{2} e^{B_t}  dt= \frac{1}{2}X_t\,dt + X_t\,dB_t
\]

\item We begin to observe that
\[
X_t =g(t,B_t)= \dfrac{B_t}{1+t}, \, B_0=0 \implies \frac{\partial g}{\partial t}(t,x)=\dfrac{-B_t}{(1+t)^2}, \, \frac{\partial g}{\partial x}(t,B_t)=\dfrac{1}{1+t}, \, \frac{\partial^2 g}{\partial x^2}(t,B_t)= 0 
\]
\[
dX_t=\frac{\partial g}{\partial t}(t,B_t)\,dt+\frac{\partial g}{\partial x}(t,B_t)\,dB_t+\frac{1}{2}\frac{\partial^2 g}{\partial x^2}(t,B_t)\cdot(dB_t)^2=
\]
\[
\dfrac{-B_t}{(1+t)^2}dt+ \dfrac{1}{1+t}  \cdot dB_t  = -\frac{1}{1+t}X_t\,dt + \frac{1}{1+t}\,dB_t
\]

\item We begin to observe that

\[
X_t =g(t,B_t)= \sin(B_t), \,B_0=a \in \bigl(-\frac{\pi}{2},\frac{\pi}{2}\bigr) \implies 
\]
\[
\frac{\partial g}{\partial t}(t,x)=0, \, \frac{\partial g}{\partial x}(t,B_t)=\cos(B_t), \, \frac{\partial^2 g}{\partial x^2}(t,B_t)= -\sin(B_t) 
\]
\[
dX_t=\frac{\partial g}{\partial t}(t,B_t)\,dt+\frac{\partial g}{\partial x}(t,B_t)\,dB_t+\frac{1}{2}\frac{\partial^2 g}{\partial x^2}(t,B_t)\cdot(dB_t)^2=
\]
\[
\cos(B_t)  \cdot dB_t  -\frac{1}{2}\sin(B_t) \cdot(dB_t)^2=  \sqrt{1-\sin(B_t)^2}\cdot dB_t  -\frac{1}{2}\sin(B_t) dt= -\frac{1}{2}X_t\,dt + \sqrt{1-X_t^2}\,dB_t
\]
Which holds for $t < \inf\{s>0: B_s \notin \bigl[-\tfrac{\pi}{2},\tfrac{\pi}{2}\bigr]\}$.

\item We begin to observe that
\[
(X_1(t),X_2(t)) = (g_1(t,B_t),g_1(t,B_t))=(t, e^{t}B_t) \implies
\]
\[
\frac{\partial g_1}{\partial t}(t,X)=1, \, \frac{\partial g_2}{\partial t}(t,B_t)=e^{t}B_t, \, \frac{\partial g_1}{\partial x}(t,B_t)=0, \frac{\partial g_2}{\partial x}(t,B_t)=e^t,
\]
\[
\frac{\partial g_1}{\partial x^2}(t,B_t) = 0, \, \frac{\partial g_2}{\partial x^2}(t,B_t)=0
\]
Then we get that
\[
dX_1=1\,dt, \, dX_2=e^tB_t\,dt+e^t\,dB_{t}=X_2 \, dt + e^{X_1} d B_t
\]
\item We begin to observe that
\[
(X_1(t),X_2(t)) = (g_1(t,B_t),g_1(t,B_t))=(\cosh(B_t), \sinh(B_t))=\left(\dfrac{e^{B_t}+e^{-B_t}}{2},\,\dfrac{e^{B_t}-e^{-B_t}}{2}\right)
 \implies
\]

\[
\frac{\partial g_1}{\partial t}(t,X)=0, \, \frac{\partial g_2}{\partial t}(t,B_t)=0, \, \frac{\partial g_1}{\partial x}(t,B_t)=\sinh(B_t), \frac{\partial g_2}{\partial x}(t,B_t)=\cosh(B_t)
\]
\[
\frac{\partial g_1}{\partial x^2}(t,B_t) = \cosh(B_t), \, \frac{\partial g_2}{\partial x^2}(t,B_t)=\sinh(B_t)
\]
Then we get that
\[
dX_1=\sinh(B_t)\,dB_{t}+\frac{1}{2}\cosh(B_t)\,dB_{t}\,dB_{t}=\sinh(B_t)\,dB_{t}+\frac{1}{2}\cosh(B_t)\,dt = X_2 \,dB_{t} + \frac{1}{2}X_1\,dt
\]
\[
dX_1=\cosh(B_t)\,dB_{t}+\frac{1}{2}\sinh(B_t)\,dB_{t}\,dB_{t}=\cosh(B_t)\,dB_{t}+\frac{1}{2}\sinh(B_t)\,dt = X_1 \,dB_{t} + \frac{1}{2}X_2\,dt
\]

\end{enumerate}



\end{solution}


% =======================
% Chapter 5, Exercise 4*
% =======================
\setchapter{5}
\setexercise{4}
\begin{exercise}
Solve the following stochastic differential equations:

\begin{enumerate}[label=(\roman*)]
\item
\[
\begin{bmatrix}
dX_1\\
dX_2
\end{bmatrix}
=
\begin{bmatrix}
1\\
0
\end{bmatrix}dt
+
\begin{bmatrix}
1 & 0\\
0 & X_1
\end{bmatrix}
\begin{bmatrix}
dB_1\\
dB_2
\end{bmatrix}.
\]

\item
\[
dX_t = X_t\,dt + dB_t.
\]
(Hint: Multiply both sides with ``the integrating factor'' $e^{-t}$ and compare with $d(e^{-t}X_t)$)

\item
\[
dX_t = -X_t\,dt + e^{-t}\,dB_t.
\]
\end{enumerate}

\end{exercise}

\begin{solution}

\begin{enumerate}[label=(\roman*)]


\item
We take the integral form
\[
X_1(t)=\int_0^tdX_1 = \int_0^t ds + \int_0^tdB_1=X_1(0)+t+B_1(t)
\]
Which gives us that
\[
X_2(t)=\int_0^tdX_2 = \int_0^t X_1(0)+s+B_1(s) dB_2=X_2(0)+X_1(0)B_2(t)+\int_0^t s dB_2  +\int_0^t B_1(s) dB_2
\]
\item
We take the hint and rewrite the following
\[
e^{-t}dX_t = e^{-t}X_t\,dt + e^{-t}dB_t \implies e^{-t}\,dX_t- e^{-t}X_t\,dt=  e^{-t}dB_t
\]
We recognize that $d(e^{-t}X_t)=e^{-t}\,dX_t- e^{-t}X_t\,dt$ thus we get that
\[
\int_0^t d(e^{-s}X_s) ds = \int_0^t e^{-s}dB_s \implies X_t=X_0e^{t}+\int_0^t e^{t-s}dB_s
\]

\item
\[
dX_t = -X_t\,dt + e^{-t}\,dB_t \implies e^{t}dX_t+e^{t}X_t\,dt =  1\,dB_t 
\]
We observe that $e^{t}dX_t+e^{t}X_t\,dt=d(e^{t}X_t)$
\[
\int_0^t d(e^{s}X_s) \,ds = \int_0^t 1\,dB_s \implies X_t=(B_t+X_0)e^{-t}
\]
\end{enumerate}

\end{solution}

\setchapter{5}
\setexercise{5}
\begin{exercise}
\begin{enumerate}[label=\alph*)]
\item Solve the \emph{Ornstein-Uhlenbeck equation} (or \emph{Langevin equation})
\[
dX_t=\mu X_t\,dt+\sigma\,dB_t
\]
where $\mu,\sigma$ are real constants, $B_t\in\mathbb{R}$.

The solution is called the \emph{Ornstein-Uhlenbeck process}. (Hint: See Exercise 5.4 (ii).)

\item Find $\E[X_t]$ and $\Var[X_t]:=\E\bigl[(X_t-\E[X_t])^2\bigr]$.
\end{enumerate}
\end{exercise}

\begin{solution}

\begin{enumerate}[label=\alph*)]

\[
e^{\mu t}dX_t=\mu e^{\mu t}X_t\,dt+\sigma e^{\mu t}\,dB_t \implies d(X_te^{\mu t})=\sigma e^{\mu t}\,dB_t
\]
Thus we integrate again
\[
\int_0^t d(X_te^{-\mu s}) \,ds = X_te^{\mu t}-X_0=\int_0^t \sigma e^{-\mu s}\,dB_s \implies
\]
\[
X_t=X_0e^{\mu t}+\int_0^t \sigma e^{-\mu (s-t)}\,dB_s
\]

 Find $\E[X_t]$ and $\Var[X_t]:=\E\bigl[(X_t-\E[X_t])^2\bigr]$.

\[
\E[X_t]=\E[X_0e^{\mu t}+\int_0^t \sigma e^{-\mu (s-t)}\,dB_s]=\E[X_0]e^{\mu t}+\sigma \E[\int_0^t  e^{-\mu (s-t)}\,dB_s]=\E[X_0]e^{\mu t}
\]
Then we want to find
\[
\Var[X_t]:=\E\bigl[(X_t-\E[X_t])^2\bigr]=\E\bigl[X_t^2\bigr]-(\E[X_0]e^{\mu t})^2
\]
Thus we solve the
\[
\E\bigl[X_t^2\bigr]=\E\bigl[(X_0e^{\mu t}+\int_0^t \sigma e^{-\mu (s-t)}\,dB_s)^2\bigr]=
\]
\[
\E\bigl[X_0^2\bigr]e^{2\mu t}+2\E\bigl[X_0e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s\bigr]+\E\bigl[(\int_0^t \sigma e^{-\mu (s-t)}\,dB_s)^2\bigr]=
\]
\[
\E\bigl[X_0^2\bigr]e^{2\mu t}+2\E\bigl[X_0e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s\bigr]+\E\bigl[\int_0^t (\sigma e^{-\mu (s-t)})^2\,dt\bigr]=
\]
\[
\E\bigl[X_0^2\bigr]e^{2\mu t}+2\E\bigl[X_0e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s\bigr]+\frac{\sigma^2}{2\mu}\bigl(e^{2\mu t}-1\bigr)
\]
We know analyze this
\[
\E\bigl[X_0e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s\bigr]=\E\bigl[\E\bigl[X_0e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s \mid \mathcal{F}_0\bigr]\bigr]=
\]
\[
\E\bigl[X_0\E\bigl[e^{\mu t} \int_0^t \sigma e^{-\mu (s-t)}\,dB_s \mid \mathcal{F}_0\bigr]\bigr]=\E\bigl[X_0 \cdot 0\bigr]=0
\]
Thus we get that
\[
\Var[X_t]=(\E\bigl[X_0^2\bigr]-\E[X_0]^2)e^{2\mu t}+\frac{\sigma^2}{2\mu}\bigl(e^{2\mu t}-1\bigr)=\Var\bigl[X_0e^{\mu t}\bigr]+\frac{\sigma^2}{2\mu}\bigl(e^{2\mu t}-1\bigr)
\]
\end{enumerate}

\end{solution}


\setchapter{5}
\setexercise{6}
\begin{exercise}
Solve the stochastic differential equation
\[
dY_t=r\,dt+\alpha Y_t\,dB_t
\]
where $r,\alpha$ are real constants, $B_t\in\mathbb{R}$.
(Hint: Multiply the equation by the `integrating factor')
\[
F_t=\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr)
\]
\end{exercise}

\begin{solution}
\[
\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr)dY_t=\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr)r\,dt+\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr)\alpha Y_t\,dB_t \implies 
\]
\[
d(\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr) Y_t)=\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr)r\,dt \implies
\]
\[
\exp\Bigl(-\alpha B_t+\tfrac12\alpha^2 t\Bigr) Y_t=exp\Bigl(\alpha B_t-\tfrac12\alpha^2 t\Bigr)(Y_0+ \int_0^t \exp\Bigl(-\alpha B_s+\tfrac12\alpha^2 s\Bigr)r\,ds 
\]

\end{solution}

\setchapter{5}
\setexercise{9}
\begin{exercise}
Show that there is a unique strong solution $X_t$ of the $1$-dimensional stochastic differential equation
\[
dX_t=\ln(1+X_t^2)\,dt+\mathbbm{1}_{\{X_t>0\}}X_t\,dB_t,
\qquad
X_0=a\in\mathbb{R}.
\]
\end{exercise}

\begin{solution}
We then have to check that the functions described satisfy Theorem $5.2.1$. Where
\[
\sigma(t,x)=\begin{cases}
    x, \, x >0 \\
    0, \, x\leq 0
\end{cases} \qquad b(t,x)=\ln(1+x^2)
\]
It then has to satisfy the two stated conditions below
\[
\exists C \text{ s.t } |\sigma(t,x)|+ |b(t,x)| \leq C (1+|x|), \,  \forall x \in \R, \,  t \in [0,T] 
\]
\[
\exists D \text{ s.t } |b(t,x)-b(t,y)|+|\sigma(t,x)-\sigma(t,x)| \leq D |x-y|, \,  \forall x \in \R, \,  t \in [0,T]
\]
We observe that tat for $\sigma$ we have that
\[
|\sigma(t,x)|\leq |x|, \, \sigma(t,x)-\sigma(t,x) = \begin{cases}
    x-y, \, x>0 \text{ and } y>0 \\
    x, \, x>0 \text{ and } y\leq 0 \\
    -y, \, x\leq 0 \text{ and } y> 0
\end{cases} \implies |\sigma(t,x)-\sigma(t,x)| \leq |x-y|
\]
Then for $b$ we have that
\[
|b(t,x)| \leq |x| \text{ since } \forall t \geq 0,  \, f(t)=t-\ln(1+t^2)  \implies f'(t)=1-\frac{2t}{1+t^2}=\frac{(t+1)^2}{1+t^2}\geq 0, \, f(0)=0
\]
We will use mean value theorem and observe that $\frac{d}{dx}\ln(1+x^2) = \frac{2x}{1+x^2} $ where $|\frac{2x}{1+x^2}| \leq 1 \implies$
\[
|\ln(1+x^2)-\ln(1+y^2)|=|f'(c)(x-y)|\leq 1 |x-y|
\]
Which means that we can take $C=D=2$ so we can conclude by the Theorem $5.2.1$ that the solution is adapted to $\mathcal{F}^Z_t \implies $ strong solution.


\begin{figure}[H]
\centering
\includegraphics[width=1.03\textwidth]{Nurmerical solutions/sde_paths_density_plot.png}
\caption{Simulated SDE paths with density, percentiles, and $+1$ standard deviation bands.}
\end{figure}

\end{solution}


\end{document}

